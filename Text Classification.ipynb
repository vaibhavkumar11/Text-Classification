{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from the 20 newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load files from the 20 newsgroup directory \n",
    "news = load_files('./mini_newsgroups/')\n",
    "news.target_names #List of all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(news.data, news.target, test_size =0.3, random_state = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing training data to remove stopwords, punctuation marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all stopwords to be removed from document\n",
    "stopwords = {\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \n",
    "             \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \n",
    "             \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \n",
    "             \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \n",
    "             \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \n",
    "             \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \n",
    "             \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \n",
    "             \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \n",
    "             \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \n",
    "             \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\",\n",
    "             \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\",\n",
    "             \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
    "             \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
    "             \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\",\n",
    "             \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "             \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \n",
    "             \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \n",
    "             \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \n",
    "             \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\"\n",
    "             , \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n",
    "             \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \n",
    "             \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \n",
    "             \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "             \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \n",
    "             \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \n",
    "             \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \n",
    "             \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \n",
    "             \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \n",
    "             \"yourself\", \"yourselves\", \"the\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    total_chars = []  # To save an array of words after processing\n",
    "    for i in range(len(data)):\n",
    "        words = data[i].decode(\"windows-1252\") # decode the given sentence from bytes format to string format\n",
    "        all_chars = words.split(' ')\n",
    "        tokenize_word = []\n",
    "        # To apply preprocess on a decoded sentence\n",
    "        for x in range(len(all_chars)):\n",
    "            word = ''\n",
    "            # Removing all meta data present in document\n",
    "            word = all_chars[x].replace('\\n','')\n",
    "            word = word.replace('\\r','')\n",
    "            word = word.replace('\\t','')\n",
    "            # Removing punctuation marks from document \n",
    "            word = word.replace(',','')\n",
    "            word = word.replace('(','')\n",
    "            word = word.replace(').','')\n",
    "            word = word.replace(')','')\n",
    "            word = word.replace('.','')\n",
    "            word = word.replace('\"','')    \n",
    "            \n",
    "            # Converting whole document to lower case\n",
    "            word = word.lower()\n",
    "            \n",
    "            # If after the preprocessing any word is blank or has word length<3 or only digits remain in it\n",
    "            if word=='' or word.isdigit() or len(word)<3:\n",
    "                continue\n",
    "            \n",
    "            # If word is not a stop word then used to form array\n",
    "            elif word not in stopwords:\n",
    "                tokenize_word.append(word)\n",
    "        \n",
    "        total_chars.append(tokenize_word)\n",
    "    \n",
    "    return total_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert the preprocessed 2D-list into a 1D-list so that frequency of words can be calculated\n",
    "\n",
    "def flatten(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            \n",
    "            if word=='':\n",
    "                continue\n",
    "            else:\n",
    "                new_data.append(word)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212606"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To apply preprocessing and flattening on training data\n",
    "\n",
    "doc_list = preprocess(X_train)\n",
    "bag_of_words = flatten(doc_list)\n",
    "len(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list into a numpy array\n",
    "np_bag_of_words = np.asarray(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get frequency of each individual word, Bag of words with Term Frequency\n",
    "word, freq  = np.unique(np_bag_of_words, return_counts=True)\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sort both arrays together based on frequency, zip function is used to map similar index of multiple containers\n",
    "# so that it can be used as a single identity\n",
    "\n",
    "mapped = list(zip(word, freq)) #Uses the zip function and converts the tupple into list\n",
    "mapped = sorted(mapped, key=lambda wrd: wrd[1], reverse=True) #Sorts the list based on Term Frequency in descending order\n",
    "word, freq = zip(*(mapped)) # Stores the unzipped list in 2 separate lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing  top 5000 words to make dictionary with words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract top 5000 words with highest frequencies to be used as features\n",
    "no_of_words = 5000\n",
    "features = word[0:no_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to form a dictionary with frequencies of all the words in the document\n",
    "def form_dictionary(doc_list):\n",
    "    dictionary = {}\n",
    "    sentence_num = 1\n",
    "    for each_sentence in doc_list:\n",
    "        dictionary[sentence_num] = {}\n",
    "        for each_word in each_sentence:\n",
    "            dictionary[sentence_num][each_word] = dictionary[sentence_num].get(each_word, 0) + 1    \n",
    "        sentence_num += 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To form a 2D-array with the frequency of every word in our features list stored individually for every document\n",
    "def matrix(dictionary):\n",
    "    final_matrix = []\n",
    "    for k in dictionary.keys():\n",
    "        row = []\n",
    "        for f in features:\n",
    "            if(f in dictionary[k].keys()):\n",
    "            #if word f is present in the dictionary of the document as a key, its value is copied\n",
    "            #this gives us no. of occurences\n",
    "                row.append(dictionary[k][f]) \n",
    "            else:\n",
    "            #if not present, the no. of occurences is zero\n",
    "                row.append(0)\n",
    "        final_matrix.append(row)\n",
    "    return final_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forms dictionary for training data\n",
    "dictionary = form_dictionary(doc_list)\n",
    "\n",
    "# Forms 2d-list with frequencies of every feature in training data\n",
    "X_train = matrix(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert the 2D-lists into numpy 2D-arrays\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying preprocessing and making dictionary for features on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To perform preprocessing on testing data\n",
    "test_chars = preprocess(X_test)\n",
    "len(test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with frequency of each word in testing data\n",
    "dictionary_test = form_dictionary(test_chars)\n",
    "# 2D-list with frequencies of every feature in testing data\n",
    "X_test = matrix(dictionary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert to numpy arrays\n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using sklearn MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7583333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.65      0.68        34\n",
      "          1       0.63      0.50      0.56        34\n",
      "          2       0.48      0.62      0.54        26\n",
      "          3       0.74      0.76      0.75        33\n",
      "          4       0.79      0.76      0.77        29\n",
      "          5       0.95      0.62      0.75        32\n",
      "          6       0.80      0.89      0.84        27\n",
      "          7       0.79      0.91      0.85        33\n",
      "          8       0.84      0.90      0.87        29\n",
      "          9       0.79      0.85      0.81        26\n",
      "         10       0.96      0.80      0.87        30\n",
      "         11       0.91      0.88      0.90        34\n",
      "         12       0.65      0.68      0.67        25\n",
      "         13       0.88      0.92      0.90        25\n",
      "         14       0.92      0.79      0.85        28\n",
      "         15       0.78      0.97      0.86        29\n",
      "         16       0.67      0.69      0.68        35\n",
      "         17       0.91      0.88      0.90        34\n",
      "         18       0.51      0.68      0.58        28\n",
      "         19       0.61      0.48      0.54        29\n",
      "\n",
      "avg / total       0.77      0.76      0.76       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using scratch implementation of Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT FUNCTION\n",
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    \n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        num_features = X_train.shape[1]\n",
    "        \n",
    "        for j in range(num_features):\n",
    "            result[current_class][features[j]] = {}\n",
    "            result[current_class][features[j]] = X_train_current[:,j].sum()\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds invidual log probabilities of every word for a given class\n",
    "def log_probablity(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    number_of_words = len(x)\n",
    "    \n",
    "    for j in range(number_of_words):\n",
    "        if(x[j] in dictionary[current_class].keys()):\n",
    "            xj = x[j]\n",
    "            count_current_class_equal_xj = dictionary[current_class][xj] + 1\n",
    "            count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class].keys())\n",
    "            current_xj_prob = np.log(count_current_class_equal_xj) - np.log(count_current_class)\n",
    "            output = output + current_xj_prob\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the best class for the given document in testing data\n",
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -10000000\n",
    "    best_class = -1\n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = log_probablity(dictionary, x, current_class)\n",
    "        if(p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "            \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    Y_predict = []\n",
    "    for x in X_test:\n",
    "        y_predicted = predictSinglePoint(dictionary, x)\n",
    "        Y_predict.append(y_predicted)\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_scratch = fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here X_test has to be changed as now we need an array with the each row(every document) store alll the words\n",
    "# it has after preprocessing. Here the individual frequency is not being used\n",
    "X_test = []\n",
    "for key in dictionary_test.keys():\n",
    "    all_words = dictionary_test[key].keys()\n",
    "    all_words = list(all_words)\n",
    "    X_test.append(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = np.asarray(predict(dictionary_scratch, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5066666666666667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.56      0.68        34\n",
      "          1       0.18      0.94      0.30        34\n",
      "          2       1.00      0.12      0.21        26\n",
      "          3       1.00      0.18      0.31        33\n",
      "          4       0.88      0.24      0.38        29\n",
      "          5       0.73      0.34      0.47        32\n",
      "          6       1.00      0.44      0.62        27\n",
      "          7       1.00      0.18      0.31        33\n",
      "          8       0.90      0.31      0.46        29\n",
      "          9       0.83      0.58      0.68        26\n",
      "         10       1.00      0.60      0.75        30\n",
      "         11       0.93      0.79      0.86        34\n",
      "         12       1.00      0.08      0.15        25\n",
      "         13       0.65      0.96      0.77        25\n",
      "         14       0.80      0.43      0.56        28\n",
      "         15       0.80      0.97      0.88        29\n",
      "         16       0.71      0.29      0.41        35\n",
      "         17       0.96      0.79      0.87        34\n",
      "         18       0.20      0.93      0.33        28\n",
      "         19       0.67      0.34      0.45        29\n",
      "\n",
      "avg / total       0.80      0.51      0.52       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, predicted_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
